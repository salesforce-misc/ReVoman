= ReṼoman (Rev-Woman)
Gopal S Akshintala <gopalakshintala@gmail.com>
:Revision: 1.0
ifdef::env-github[]
:tip-caption: :bulb:
:note-caption: :information_source:
:important-caption: :heavy_exclamation_mark:
:caution-caption: :fire:
:warning-caption: :warning:
endif::[]
:toc:
:toc-placement!:
:sourcedir: src/main/kotlin
:testdir: src/integrationTest/java
:pmtemplates: src/integrationTest/resources/pm-templates
:imagesdir: docs/images
:prewrap!:
:revoman-version: 0.21.2

____

Re - Request/Response

Ṽ - Validation

____

'''


*ReVoman* is an API automation tool for JVM (Java/Kotlin) from the API-first SaaS company, Salesforce. It lets you execute a Postman collection in a JVM program/test.

'''

[.lead]
To start with, think of it as Postman for JVM (Java/Kotlin); that emulates this *Run* button on a collection through a Java program.
But it's even better

image::postman-run.png[]


== Artifacts

[.lead]
Maven
[source,xml,subs=attributes+]
----
<dependency>
  <groupId>com.salesforce.revoman</groupId>
  <artifactId>revoman</artifactId>
  <version>{revoman-version}</version>
</dependency>
----
[.lead]
Bazel
[source,bzl,subs=attributes+]
----
"com.salesforce.revoman:revoman"
----
[.lead]
Gradle Kts
[source,kts,subs=attributes+]
----
implementation("com.salesforce.revoman:revoman:{revoman-version}")
----

toc::[]

== Why ReVoman?

=== The Problem

* The majority of JVM SaaS applications are REST-based. But the API automation (like Integration Tests) is done with lots and lots of code.
* This automation competes on cognitive complexity and learning curve with the Prod code, and mostly, automation wins.
* We have tools like Postman for API automation, but they don't live with the code and need altogether a different skill set to write and maintain and don't work well with JVM
* On top of that, we have a mul-*T*-verse of FTests, E2E tests, each with its own frameworks, tools, and internal utilities.
* You need altogether a different strategy and a new set of tools to learn and implement, to cover the same flow through an E2E test, which demands an almost equal amount of code.

image::cognitive-complexity.png[]

____

How _productive_ would it be, if all you need to execute an FTest or an E2E test is the same Postman collection,
that you anyway would have created for your manual testing?

____

=== API automation with _ReṼoman_

Let’s check out how you can perform *Template-Driven-Testing* with ReVoman:

== API

=== Input

You can kick all this off with this simple API call, supplying a config

[source,java,indent=0,options="nowrap"]
----
ReVoman.revUp(
  Kick.configure()
    ...
    .off())
----

Let’s check out how to build this config:

ifdef::env-github[]

[source,java,indent=0,options="nowrap"]
.link:{testdir}/com/salesforce/revoman/integration/core/pq/PQE2ETest.java[PQE2ETest.java, tag=pq-e2e-with-revoman-config-demo]
----
ReVoman.revUp( // <1>
    Kick.configure()
        .templatePath("pm-templates/pq/pq-with-rc.postman_collection.json") // <2>
        .environmentPath("pm-templates/pq/pq-env.postman_environment.json") // <3>
        .dynamicEnvironment( // <4>
            Map.of(
                "$quoteFieldsToQuery", "LineItemCount, CalculationStatus",
                "$qliFieldsToQuery", "Id, Product2Id",
                "$qlrFieldsToQuery", "Id, QuoteId, MainQuoteLineId, AssociatedQuoteLineId"))
        .customDynamicVariable( // <5>
            "$quantity", ignore -> String.valueOf(Random.Default.nextInt(10) + 1))
        .requestConfig( // <6>
            unmarshallRequest(
                ASYNC_STEP_NAMES,
                PlaceQuoteInputRepresentation.class,
                adapter(PlaceQuoteInputRepresentation.class)))
        .hooks( // <7>
            post(
                "password-reset",
                (stepName, rundown) ->
                    LOGGER.info(
                        "Step count: {} executed including this step: {}",
                        rundown.stepNameToReport.size(),
                        stepName)),
            pre(
                "pq-create-with-bundles",
                (stepName, requestInfo, rundown) ->
                    LOGGER.info(
                        "Step count: {} executed before this step: {}",
                        rundown.stepNameToReport.size(),
                        stepName)),
            pre(
                ASYNC_STEP_NAMES,
                (stepName, requestInfo, rundown) -> {
                  final var pqInputRep =
                      requestInfo.<PlaceQuoteInputRepresentation>getTypedTxObj();
                  assertThat(pqInputRep).isNotNull();
                  if ("pq-create: qli+qlr (skip-pricing)"
                      .equals(pqInputRep.getGraph().getGraphId())) {
                    LOGGER.info("Skip pricing for step: {}", stepName);
                    rundown.mutableEnv.set("$pricingPref", PricingPref.Skip.toString());
                  } else {
                    rundown.mutableEnv.set("$pricingPref", PricingPref.System.toString());
                  }
                }),
            post("query-quote-and-related-records", PQE2ETest::assertAfterPQCreate),
            post(
                ASYNC_STEP_NAMES,
                (stepName, rundown) -> {
                  LOGGER.info(
                      "Waiting after Step: {} for the Quote: {} to get processed",
                      stepName,
                      rundown.mutableEnv.getString("quoteId"));
                  // ! CAUTION 10/09/23 gopala.akshintala: This test can be flaky until
                  // polling is implemented
                  Thread.sleep(20000);
                }))
        .haltOnAnyFailureExceptForSteps(unsuccessfulStepsException) // <8>
        .responseConfig( // <9>
            unmarshallSuccessResponse("quote-related-records", CompositeResponse.class),
            validateIfSuccess( // <9.1>
                ASYNC_STEP_NAMES,
                PlaceQuoteOutputRepresentation.class,
                validatePQSuccessResponse),
            validateIfFailed(
                FAILURE_STEP_NAMES,
                PlaceQuoteOutputRepresentation.class,
                validatePQErrorResponse))
        .insecureHttp(true) // <10>
        .off()); // Kick-off
----
<1> `revUp` is the method to call passing a configuration, built as below
<2> Supply the path (relative to resources) to the Template Collection JSON file
<3> Supply the path (relative to resources) to the Environment JSON file
<4> Supply any dynamic environment
<5> Plug Custom dynamic variables that are prepared at runtime
<6> You can provide a strong type for your request to be marshalled into. This shall be passed onto which executing pre-hooks
<7> You can set up a pre- / post-hook around a Step via `hooks`, which can help as callbacks, especially for Async operations.
** These runs despite the step are successful or failed. The entire rundown till that step shall be provided to the hook as a parameter
<8> It lets you be in charge of the Step Orchestration by letting you configure a pass-list of steps to ignore for failure
<9> Provide configuration to unmarshall/deserialize response JSON into strong types
** ReVoman supports all data types within or outside the core without any extra annotations.
** `unmarshallErrorResponse` can be used for error Types
<9.1> Supply your validations/assertions to be run on a step response
** You can leverage the power of https://github.com/salesforce-misc/Vador[*Vador*] to write config-driven validations and supply them to ReVoman `validateIfSuccess`.
** Because both these tools are from the same Development team, you should see homogeneous patterns and seamless Integration and support.
** `validateIfError` can be used for error Types
<10> [Not for Prod] Ignore Java cert issues when firing Http calls

endif::[]
ifndef::env-github[]

[source,java,indent=0,options="nowrap"]
.link:{testdir}/com/salesforce/revoman/integration/core/pq/PQE2ETest.java[PQE2ETest.java, tag=pq-e2e-with-revoman-config-demo]
----
include::{testdir}/com/salesforce/revoman/integration/core/pq/PQE2ETest.java[tag=pq-e2e-with-revoman-config-demo]
----
<1> `revUp` is the method to call passing a configuration, built as below
<2> Supply the path (relative to resources) to the Template Collection JSON file
<3> Supply the path (relative to resources) to the Environment JSON file
<4> Supply any dynamic environment
<5> Plug Custom dynamic variables that are prepared at runtime
<6> You can provide a strong type for your request to be marshalled into. This shall be passed onto which executing pre-hooks
<7> You can set up a pre- / post-hook around a Step via `hooks`, which can help as callbacks, especially for Async operations.
** These runs despite the step are successful or failed. The entire rundown till that step shall be provided to the hook as a parameter
<8> It lets you be in charge of the Step Orchestration by letting you configure a pass-list of steps to ignore for failure
<9> Provide configuration to unmarshall/deserialize response JSON into strong types
** ReVoman supports all data types within or outside the core without any extra annotations.
** `unmarshallErrorResponse` can be used for error Types
<9.1> Supply your validations/assertions to be run on a step response
** You can leverage the power of https://github.com/salesforce-misc/Vador[*Vador*] to write config-driven validations and supply them to ReVoman `validateIfSuccess`.
** Because both these tools are from the same Development team, you should see homogeneous patterns and seamless Integration and support.
** `validateIfError` can be used for error Types
<10> [Not for Prod] Ignore Java cert issues when firing Http calls

endif::[]

=== Output

After all this, you receive back a detailed Rundown of all the steps with all the Request-Response data. You get Strong types for the ones you are interested in so that you can run more assertions on top of the run.

[source,kotlin,indent=0,options="nowrap"]
----
Rundown(
  stepNameToReport: Map<String, StepReport>,
  environment: PostmanEnvironment)

StepReport(
    val status: String,
    val requestInfo: Either<RequestFailure, TxInfo<Request>>?,
    val preHookFailure: PreHookFailure?,
    val responseInfo: Either<ResponseFailure, TxInfo<Response>>?,
    val postHookFailure: PostHookFailure?,
    val postmanEnvironmentSnapshot: PostmanEnvironment<Any?>
)
----

Here is what a debugger view of a Rundown looks like:

image:full-report.png[Rundown of all steps]

Along with all the environment

image:mutable-env.png[Mutable environment after the execution completion]

This report has everything you need to know about what happened during each step execution,
along with environment snapshot during that step execution.

image:step-report.png[Step Report]

You can run further assertions on top of this report

=== Perf

This entire execution of **70 steps**, which include **10 async steps**, took a mere *3.6 min* on localhost.
This can be much better on auto-build environments.
(TBD: Capture the avg time taken on auto-builds)

image:ftest-console.png[Localhost Test time on FTest console]

WARNING: ReVoman internally is very light-weight and the execution times are proportional
to how your server responds or your network speeds.

=== Low-code

____

*Here you go, an link:{testdir}/com/salesforce/revoman/integration/core/pq/PQE2ETest.java[E2E test]*

____

[.lead]
This demonstrates *a Low-code FTest that tests E2E*, Transparent and Low in Cognitive Complexity.

NOTE: *FTest with ReVoman:* Lines of Code: *136 (89% low-code)*

The amount of code needed for your FTest suite is drastically down by *89%*

== Postman collection on Git

* Now that ReVoman hooks these templates into auto-builds or CI/CD, they always stay up to date, otherwise, Yoda makes sure they are with TFs.
* The entire Postman collection guards each *check-in* in the form of a Test suite.
* Any day, you can find a Postman collection for every feature you need to test, right in your VCS. Developers can import these templates directly from VCS for manual testing. This comes in very handy during FF/RF/Cross team blitz.
* With ReVoman, you no more need a release task to keep your Postman collections up-to-date.

== Internal Orchestration

image::revoman-flow.png[ReVoman flow]

* It reads the environment JSON provided into in-memory.
* Then it reads and Inflates each static template in the collection, replacing variables at runtime from the in-memory environment
* It uses this information to Fire an HTTP request.
* It reads the response and executes Postscript JS on the response and updates the in-memory environment.
* It unmarshals the response into Strong JVM types as per the `responseConfig` supplied and lets you run *Type-safe* validations on the strong-type and fails-fast at first failure.
* Finally, if configured, the pre- /post-hook for the step gets triggered by passing the entire runDown
* The iteration continues for all the steps in the template collection

== Core Support

* First-class Salesforce core support
* Convert your persona-based Postman templates into JVM FTests and E2E tests that can be executed in auto-builds.
* Supports Steps involving Async operations

== Unified testing strategy

* Bring a *Unified &amp; Simplified Test strategy* across the mul-**T**-verse (FTests, E2E Tests, and Manual testing with Postman). This is a generic tool, and just by changing the template, the same config/pattern can be reused for any feature flow agnostic of it being an FTest or E2E test
* Transparency and better Traceability of issues
* This forces engineers to think like *API-first customers* while writing tests.
*FTest Data setup:* You can use the ReVoman for the FTest data setup too. This eliminates the need for different teams to write their own internal utilities for data setup.

== Future

[.lead]
The future looks bright

* *It's built with extensibility* in mind. It can easily be extended to support downloadable *Kaiju* templates too.
** You should be able to run Kaiju availability tests right from your IDE and debug them too

== FAQs

=== Is Debugging difficult with ReVoman?

* IDE debug points in the Prod code work as expected while running the test.
* Coming to FTest code, we debug when we don't understand what's going on in the code.
* Debugging necessarily doesn't have to be with a debug point in the IDE.
* To be able to debug, a developer needs to be informed about what went wrong and he/she should have ways to try and test an isolated portion of the run.
* In the case of ReVoman, you have the whole Postman collection at your disposal along with the Rundown. The entire test is transparent.
* This experience can be enhanced with more logging and better assertions.

=== Why not use https://learning.postman.com/docs/collections/using-newman-cli/command-line-integration-with-newman[Newman] or https://learning.postman.com/docs/postman-cli/postman-cli-overview/#comparing-the-postman-cli-and-newman[Postman CLI]?

* ReVoman may be similar to Newman or Postman CLI when it comes to executing a Postman collection, but the _similarities end there_.
* Newman/Postman CLI are built for node cannot be executed within a JVM. Even if you are able to run with some hacky way, there is no easy way to assert results.
* ReVoman is JVM first that lets you configure a lot more, and gives you back a detailed report to assert in a typesafe way
* Newman is limited and cannot be integrated into our automation model on JVM

== link:CONTRIBUTING.adoc[🙌🏼Wanna Collab & Contribute?]
